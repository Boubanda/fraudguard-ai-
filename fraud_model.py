"""
ü§ñ Mod√®le ML FraudGuard AI
D√©tection de fraude hybride: XGBoost + Isolation Forest
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

from config import *

class FraudDetector:
    """D√©tecteur de fraude hybride avec mod√®les multiples"""
    
    def __init__(self):
        # Mod√®les ML
        self.xgb_model = XGBClassifier(**XGBOOST_PARAMS)
        self.isolation_forest = IsolationForest(**ISOLATION_FOREST_PARAMS)
        
        # Preprocessing
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
        # M√©triques
        self.performance_metrics = {}
        self.feature_importance = {}
        
        print("ü§ñ FraudDetector initialis√©")
    
    def preprocess_data(self, df):
        """Preprocessing des donn√©es pour ML"""
        print("üîÑ Preprocessing des donn√©es...")
        
        # Copie pour √©viter modifications
        data = df.copy()
        
        # Traitement des variables temporelles
        data['timestamp'] = pd.to_datetime(data['timestamp'])
        data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)
        data['is_night'] = data['hour'].isin([0, 1, 2, 3, 4, 5]).astype(int)
        
        # Feature engineering avanc√©
        data['amount_log'] = np.log1p(data['amount'])
        data['velocity_risk'] = data['velocity_1h'] / (data['avg_amount_30d'] + 1)
        data['amount_deviation'] = abs(data['amount'] - data['avg_amount_30d']) / (data['std_amount_30d'] + 1)
        data['total_risk_score'] = data['geographic_risk'] + data['device_risk']
        
        # Encodage des variables cat√©gorielles
        for feature in CATEGORICAL_FEATURES:
            if feature in data.columns:
                if feature not in self.label_encoders:
                    self.label_encoders[feature] = LabelEncoder()
                    data[f'{feature}_encoded'] = self.label_encoders[feature].fit_transform(data[feature].astype(str))
                else:
                    data[f'{feature}_encoded'] = self.label_encoders[feature].transform(data[feature].astype(str))
        
        # S√©lection des features finales
        feature_columns = [
            'amount', 'amount_log', 'hour', 'day_of_week', 'month',
            'user_age', 'account_age_days', 'transaction_count_day',
            'amount_last_hour', 'amount_last_day', 'velocity_1h',
            'avg_amount_30d', 'std_amount_30d', 'geographic_risk',
            'device_risk', 'time_since_last_transaction',
            'is_weekend', 'is_night', 'velocity_risk', 
            'amount_deviation', 'total_risk_score'
        ]
        
        # Ajouter les features encod√©es
        for feature in CATEGORICAL_FEATURES:
            encoded_col = f'{feature}_encoded'
            if encoded_col in data.columns:
                feature_columns.append(encoded_col)
        
        # Garder seulement les features existantes
        available_features = [col for col in feature_columns if col in data.columns]
        X = data[available_features]
        
        print(f"‚úÖ Features utilis√©es: {len(available_features)}")
        print(f"   {available_features}")
        
        return X, data
    
    def train(self, df):
        """Entra√Ænement des mod√®les hybrides"""
        print("üöÄ D√©but de l'entra√Ænement des mod√®les...")
        
        # Preprocessing
        X, processed_data = self.preprocess_data(df)
        y = df['is_fraud']
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Normalisation
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"üìä Donn√©es d'entra√Ænement:")
        print(f"   Train: {X_train.shape[0]} samples")
        print(f"   Test: {X_test.shape[0]} samples")
        print(f"   Features: {X_train.shape[1]}")
        print(f"   Fraudes train: {y_train.sum()} ({y_train.mean()*100:.1f}%)")
        
        # === ENTRA√éNEMENT ISOLATION FOREST ===
        print("\nüå≤ Entra√Ænement Isolation Forest...")
        self.isolation_forest.fit(X_train_scaled)
        
        # Pr√©dictions anomalies
        isolation_train_pred = self.isolation_forest.decision_function(X_train_scaled)
        isolation_test_pred = self.isolation_forest.decision_function(X_test_scaled)
        
        # === HANDLING CLASS IMBALANCE AVEC SMOTE ===
        print("\n‚öñÔ∏è Gestion du d√©s√©quilibre des classes...")
        smote = SMOTE(random_state=42)
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
        
        print(f"   Avant SMOTE: {y_train.value_counts().to_dict()}")
        print(f"   Apr√®s SMOTE: {pd.Series(y_train_balanced).value_counts().to_dict()}")
        
        # === ENTRA√éNEMENT XGBOOST ===
        print("\nüöÄ Entra√Ænement XGBoost...")
        self.xgb_model.fit(
            X_train_balanced, y_train_balanced,
            eval_set=[(X_test_scaled, y_test)],
            verbose=False
        )
        
        # Importance des features
        self.feature_importance = dict(zip(X.columns, self.xgb_model.feature_importances_))
        
        # === √âVALUATION ===
        print("\nüìä √âvaluation des mod√®les...")
        self._evaluate_models(X_test_scaled, y_test, isolation_test_pred)
        
        print("‚úÖ Entra√Ænement termin√© avec succ√®s!")
        
        return self.performance_metrics
    
    def _evaluate_models(self, X_test, y_test, isolation_pred):
        """√âvaluation compl√®te des mod√®les"""
        
        # Pr√©dictions XGBoost
        xgb_proba = self.xgb_model.predict_proba(X_test)[:, 1]
        xgb_pred = self.xgb_model.predict(X_test)
        
        # Pr√©dictions Isolation Forest (convertir en probabilit√©s)
        isolation_proba = (isolation_pred - isolation_pred.min()) / (isolation_pred.max() - isolation_pred.min())
        isolation_binary = (isolation_pred < 0).astype(int)
        
        # Pr√©dictions hybrides (ensemble)
        hybrid_proba = (xgb_proba + (1 - isolation_proba)) / 2
        hybrid_pred = (hybrid_proba > 0.5).astype(int)
        
        # M√©triques pour chaque mod√®le
        models_results = {
            'XGBoost': {
                'predictions': xgb_pred,
                'probabilities': xgb_proba
            },
            'Isolation Forest': {
                'predictions': isolation_binary,
                'probabilities': 1 - isolation_proba
            },
            'Hybrid Model': {
                'predictions': hybrid_pred,
                'probabilities': hybrid_proba
            }
        }
        
        print("\nüéØ R√âSULTATS D√âTAILL√âS:")
        print("=" * 50)
        
        for model_name, results in models_results.items():
            pred = results['predictions']
            proba = results['probabilities']
            
            # M√©triques de base
            auc = roc_auc_score(y_test, proba)
            
            # Confusion matrix
            tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            
            print(f"\nüìà {model_name}:")
            print(f"   AUC: {auc:.3f}")
            print(f"   Precision: {precision:.3f}")
            print(f"   Recall: {recall:.3f}")
            print(f"   F1-Score: {f1:.3f}")
            print(f"   Specificity: {specificity:.3f}")
            print(f"   False Positive Rate: {fp/(fp+tn)*100:.1f}%")
            
            # Sauvegarder les m√©triques
            self.performance_metrics[model_name] = {
                'auc': auc,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'specificity': specificity,
                'false_positive_rate': fp/(fp+tn),
                'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}
            }
        
        # Top features importantes
        print(f"\nüîç TOP 10 FEATURES IMPORTANTES:")
        sorted_features = sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)
        for feature, importance in sorted_features[:10]:
            print(f"   {feature}: {importance:.3f}")
    
    def predict(self, transaction_data):
        """Pr√©diction temps r√©el pour une transaction"""
        
        # Convertir en DataFrame si n√©cessaire
        if isinstance(transaction_data, dict):
            df = pd.DataFrame([transaction_data])
        else:
            df = transaction_data.copy()
        
        # Preprocessing
        X, _ = self.preprocess_data(df)
        X_scaled = self.scaler.transform(X)
        
        # Pr√©dictions des deux mod√®les
        xgb_proba = self.xgb_model.predict_proba(X_scaled)[:, 1]
        isolation_score = self.isolation_forest.decision_function(X_scaled)
        
        # Normaliser isolation score
        isolation_proba = 1 / (1 + np.exp(-isolation_score))  # Sigmoid
        
        # Score hybride
        fraud_score = (xgb_proba + isolation_proba) / 2
        
        # Classification selon seuils
        if fraud_score[0] >= FRAUD_THRESHOLD_HIGH:
            risk_level = "HIGH"
            action = "BLOCK"
        elif fraud_score[0] >= FRAUD_THRESHOLD_MEDIUM:
            risk_level = "MEDIUM"
            action = "ALERT"
        elif fraud_score[0] >= FRAUD_THRESHOLD_LOW:
            risk_level = "LOW"
            action = "MONITOR"
        else:
            risk_level = "MINIMAL"
            action = "APPROVE"
        
        return {
            'fraud_score': float(fraud_score[0]),
            'xgb_score': float(xgb_proba[0]),
            'isolation_score': float(isolation_proba[0]),
            'risk_level': risk_level,
            'action': action,
            'is_fraud_predicted': fraud_score[0] > 0.5
        }
    
    def save_model(self, filepath=MODEL_PATH):
        """Sauvegarde du mod√®le complet"""
        model_data = {
            'xgb_model': self.xgb_model,
            'isolation_forest': self.isolation_forest,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'performance_metrics': self.performance_metrics,
            'feature_importance': self.feature_importance
        }
        
        joblib.dump(model_data, filepath)
        print(f"üíæ Mod√®le sauvegard√©: {filepath}")
    
    def load_model(self, filepath=MODEL_PATH):
        """Chargement du mod√®le complet"""
        try:
            model_data = joblib.load(filepath)
            
            self.xgb_model = model_data['xgb_model']
            self.isolation_forest = model_data['isolation_forest']
            self.scaler = model_data['scaler']
            self.label_encoders = model_data['label_encoders']
            self.performance_metrics = model_data['performance_metrics']
            self.feature_importance = model_data['feature_importance']
            
            print(f"‚úÖ Mod√®le charg√©: {filepath}")
            return True
        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le: {e}")
            return False

def main():
    """Fonction principale - entra√Ænement du mod√®le"""
    print("üöÄ Lancement de l'entra√Ænement FraudGuard AI")
    
    # Charger les donn√©es
    print("üìä Chargement des donn√©es...")
    try:
        df = pd.read_csv(TRANSACTIONS_FILE)
        print(f"‚úÖ {len(df)} transactions charg√©es")
    except FileNotFoundError:
        print("‚ùå Fichier de donn√©es non trouv√©. Lancez d'abord data_generator.py")
        return
    
    # Cr√©er et entra√Æner le mod√®le
    detector = FraudDetector()
    metrics = detector.train(df)
    
    # Sauvegarder le mod√®le
    detector.save_model()
    
    # Test de pr√©diction
    print("\nüß™ Test de pr√©diction:")
    sample_transaction = df.iloc[0].to_dict()
    result = detector.predict(sample_transaction)
    print(f"   Transaction test: {result}")
    
    print("\n‚úÖ Entra√Ænement termin√© avec succ√®s!")
    print(f"üìä Mod√®le sauvegard√© dans: {MODEL_PATH}")

if __name__ == "__main__":
    main()